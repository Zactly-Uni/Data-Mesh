from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

spark = SparkSession.builder.appName("SolarkatasterTransform").getOrCreate()

# ClickHouse Connection
CLICKHOUSE_HOST = "fragjetzt-staging.mni.thm.de"
CLICKHOUSE_PORT = "8123"
CLICKHOUSE_DB = "solar_data_mesh"
CLICKHOUSE_TABLE = "solarkataster"

jdbc_url = f"jdbc:clickhouse://{CLICKHOUSE_HOST}:{CLICKHOUSE_PORT}/{CLICKHOUSE_DB}"

# Spark liest Tabelle aus ClickHouse
df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", CLICKHOUSE_TABLE) \
    .option("driver", "com.clickhouse.jdbc.ClickHouseDriver") \
    .load()

print("Rows:", df.count())

# Use-case: Daten bereinigen
df_clean = df \
    .withColumn("leistung_kwp", when(col("leistung_kwp") < 0, None).otherwise(col("leistung_kwp"))) \
    .withColumn("strom_kwh", when(col("strom_kwh") < 0, None).otherwise(col("strom_kwh"))) \
    .withColumn("effizienz",
        when((col("leistung_kwp").isNotNull()) & (col("leistung_kwp") > 0),
             col("strom_kwh") / col("leistung_kwp")
        ).otherwise(None)
    )

# Optional: Filter auf sinnvolle Werte
df_clean = df_clean.filter(col("leistung_kwp").isNotNull())

# Ergebnis als CSV auf Server speichern
out_path = "file:///spark-jobs/solarkataster/output/solarkataster_clean"
out_path = "file:///tmp/solarkataster_clean"
df_clean.coalesce(1).write.mode("overwrite").option("header", "true").csv(out_path)

print("âœ… Export done:", out_path)

spark.stop()

